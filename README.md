# Project: Data Warehouse

## Introduction
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. To analyze user activity on the platform, the S3 data will be copied into Redshift, initially as staging tables, and then an ETL will create a fact table and dimension tables for each of querying data.


## S3 data
Song data: s3://udacity-dend/song_data

Log data: s3://udacity-dend/log_data
Log data json path: s3://udacity-dend/log_json_path.json

## Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

## Log Dataset
The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

Each JSON has the following columns of data:

artist
auth
firstName
gender
itemInSession
lastName
length
level
location
method
page
registration
sessionId
song
status
ts
userAgent
userId

## Schema for Song Play Analysis
The song and log data will be copied from S3 into a Redshift cluster, initially as two staging tables, staging_songs and staging_events respectively, each having all the columns verbatim from JSON, after which one fact table and four dimension tables are created by querying data from the staging tables.

Fact table:
songplays - records in event data associated with song plays i.e. records with page NextSong
columns: songplay_id (sortkey and distkey), start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dim tables:
users - users in the app 
columns: user_id (sortkey), first_name, last_name, gender, level

songs in music database
columns: song_id (sortkey), title, artist_id, year, duration

artists in music database
columns: artist_id (sortkey), name, location, lattitude, longitude

time - timestamps of records in songplays broken down into specific units
columns: start_time (sortkey), hour, day, week, month, year, weekday

Design considerations
All tables have their respective primary keys as sort keys to order each table and speed up query executions. The song_id was used as the distkey in the fact table and the songs dim table, so that any analysis looking at song data would be more efficient by grouping similar data together.

## Example queries

1. How many users are each in level?

SELECT DISTINCT
level,
COUNT(level)
FROM users
GROUP BY 1
ORDER BY 1

1 free 83
2 paid 22

2. Which artists have the most song plays as identified in the songs and log data?

SELECT DISTINCT
a.name AS artist,
COUNT(s.song_id)
FROM songplays s
INNER JOIN artists a ON s.artist_id = a.artist_id
GROUP BY 1
ORDER BY 2 DESC
LIMIT 10

1 Dwight Yoakam 37
2 Kid Cudi 10
3 Kid Cudi / Kanye West / Common 10
4 Ron Carter 9
5 Lonnie Gordon 9
6 B.o.B 8
7 Usher 6
8 Usher featuring Jermaine Dupri 6
9 Muse 6
10 Richard Hawley And Death Ramps_ Arctic Monkeys 5